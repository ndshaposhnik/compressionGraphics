{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2bd71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29890e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopKMask(x, k):\n",
    "    N = x.shape[0]\n",
    "    xSorted = sorted(list(enumerate(x)), key=lambda elem : abs(elem[1]), reverse=True)\n",
    "    mask = np.zeros(N)\n",
    "    for i in range(k):\n",
    "        mask[xSorted[i][0]] = 1\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb078917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topK(gradient0, k):\n",
    "    gradient = gradient0.copy()\n",
    "    gradient *= getTopKMask(gradient, k)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed33b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smartCompression(gradient0, probability, k, alpha=0.5):\n",
    "    gradient = gradient0.copy()\n",
    "    N = gradient.shape[0]\n",
    "    mask = getTopKMask(probability, k)\n",
    "    gradient *= mask\n",
    "    probability *= (np.ones(N) - mask * alpha)\n",
    "    probability = softmax(probability)\n",
    "    return gradient, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db27fa62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  6.,  7.,  8.,  9., 10.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(1, 11, dtype=np.double)\n",
    "topK(a, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38288443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.,  0.,  0.,  0.,  0.,  6.,  7.,  8.,  9., 10.]),\n",
       " array([1. , 2. , 3. , 4. , 5. , 3. , 3.5, 4. , 4.5, 5. ]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smartCompression(a, a, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "079eb512",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = lambda x : 1 / (1 + np.exp(-x))\n",
    "\n",
    "def gradf(theta, X, y):\n",
    "    N, dim = X.shape\n",
    "    res = np.zeros(dim)\n",
    "    for i in range(N):\n",
    "        res += (y[i] - sigma(theta @ X[i].T)) * X[i]\n",
    "    return res\n",
    "\n",
    "def calculateLoss(theta, X, y):\n",
    "    N, dim = X.shape\n",
    "    res = 0\n",
    "    for i in range(N):\n",
    "        res += y[i] * np.log(sigma(theta @ X[i].T)) + (1 - y[i]) * np.log(1 - sigma(theta @ X[i].T))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "223ecc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientDescent(gradf, theta0, X, y, max_iter=1000000, tol=1e-2, compression=None):\n",
    "    theta = theta0.copy()\n",
    "    iteration = 0\n",
    "    gradients = []\n",
    "    conv_array = []\n",
    "    loss = []\n",
    "\n",
    "    while True:\n",
    "        alpha = 0.04\n",
    "        conv_array.append(theta)\n",
    "        loss.append(calculateLoss(theta, X, y))\n",
    "        gradient0 = gradf(theta, X, y)\n",
    "        gradient = gradient0.copy()\n",
    "        gradients.append(np.linalg.norm(gradient0))\n",
    "        if compression:\n",
    "            gradient = compression(gradient)\n",
    "        theta = theta + alpha * gradient\n",
    "\n",
    "        iteration += 1\n",
    "        if np.linalg.norm(gradient0) < tol:\n",
    "            break\n",
    "        if iteration >= max_iter:\n",
    "            break\n",
    "    res = {\n",
    "        \"num_iter\": iteration,\n",
    "        \"loss\": loss,\n",
    "        \"gradients\": gradients,\n",
    "        \"tol\": np.linalg.norm(gradient),\n",
    "        \"conv_array\": conv_array,\n",
    "    }\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5507449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SmartGradientDescent(gradf, theta0, X, y, k, max_iter=1000000, tol=1e-2):\n",
    "    theta = theta0.copy()\n",
    "    iteration = 0\n",
    "    gradients = []\n",
    "    conv_array = []\n",
    "    loss = []\n",
    "    probability = None\n",
    "\n",
    "    while True:\n",
    "        alpha = 0.04\n",
    "        conv_array.append(theta)\n",
    "        loss.append(calculateLoss(theta, X, y))\n",
    "        gradient0 = gradf(theta, X, y)\n",
    "        gradient = gradient0.copy()\n",
    "        gradients.append(np.linalg.norm(gradient0))\n",
    "        \n",
    "        if iteration == 0:\n",
    "            probability = gradient.copy()\n",
    "        gradient, probability = smartCompression(gradient, probability, k)\n",
    "        theta = theta + alpha * gradient\n",
    "\n",
    "        iteration += 1\n",
    "        if np.linalg.norm(gradient0) < tol:\n",
    "            break\n",
    "        if iteration >= max_iter:\n",
    "            break\n",
    "    res = {\n",
    "        \"num_iter\": iteration,\n",
    "        \"loss\": loss,\n",
    "        \"gradients\": gradients,\n",
    "        \"tol\": np.linalg.norm(gradient),\n",
    "        \"conv_array\": conv_array,\n",
    "    }\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39341b71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
